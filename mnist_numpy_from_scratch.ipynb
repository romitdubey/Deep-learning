{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# MNIST Digit Classification (NumPy from Scratch)\n", "\n", "This notebook implements a feedforward neural network to classify handwritten digits from the MNIST dataset using only NumPy. It includes:\n", "- Manual forward and backward propagation\n", "- ReLU and Softmax activation\n", "- Cross-entropy loss\n", "- Adam optimizer\n", "- Hyperparameter tuning\n", "- Visualizations"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "from sklearn.model_selection import train_test_split\n", "from tensorflow.keras.datasets import mnist\n", "from sklearn.preprocessing import OneHotEncoder\n", "\n", "# Load data\n", "(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()\n", "\n", "# Normalize to [0, 1]\n", "X_train_full = X_train_full / 255.0\n", "X_test = X_test / 255.0\n", "\n", "# Flatten (28x28) \u2192 (784)\n", "X_train_full = X_train_full.reshape(-1, 28*28)\n", "X_test = X_test.reshape(-1, 28*28)\n", "\n", "# One-hot encode labels\n", "encoder = OneHotEncoder(sparse=False)\n", "y_train_full_oh = encoder.fit_transform(y_train_full.reshape(-1, 1))\n", "y_test_oh = encoder.transform(y_test.reshape(-1, 1))\n", "\n", "# Train/validation split\n", "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full_oh, test_size=0.1, random_state=42)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def relu(z):\n", "    return np.maximum(0, z)\n", "\n", "def relu_derivative(z):\n", "    return (z > 0).astype(float)\n", "\n", "def softmax(z):\n", "    exps = np.exp(z - np.max(z, axis=1, keepdims=True))\n", "    return exps / np.sum(exps, axis=1, keepdims=True)\n", "\n", "def cross_entropy(y_pred, y_true):\n", "    return -np.mean(np.sum(y_true * np.log(y_pred + 1e-8), axis=1))\n", "\n", "def accuracy(y_pred, y_true):\n", "    return np.mean(np.argmax(y_pred, axis=1) == np.argmax(y_true, axis=1))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class NeuralNetwork:\n", "    def __init__(self, input_size, hidden1, hidden2, output_size, lr=0.001):\n", "        self.lr = lr\n", "        self.params = {\n", "            \"W1\": np.random.randn(input_size, hidden1) * 0.01,\n", "            \"b1\": np.zeros((1, hidden1)),\n", "            \"W2\": np.random.randn(hidden1, hidden2) * 0.01,\n", "            \"b2\": np.zeros((1, hidden2)),\n", "            \"W3\": np.random.randn(hidden2, output_size) * 0.01,\n", "            \"b3\": np.zeros((1, output_size)),\n", "        }\n", "\n", "    def forward(self, X):\n", "        self.cache = {}\n", "        self.cache[\"Z1\"] = X @ self.params[\"W1\"] + self.params[\"b1\"]\n", "        self.cache[\"A1\"] = relu(self.cache[\"Z1\"])\n", "        self.cache[\"Z2\"] = self.cache[\"A1\"] @ self.params[\"W2\"] + self.params[\"b2\"]\n", "        self.cache[\"A2\"] = relu(self.cache[\"Z2\"])\n", "        self.cache[\"Z3\"] = self.cache[\"A2\"] @ self.params[\"W3\"] + self.params[\"b3\"]\n", "        self.cache[\"A3\"] = softmax(self.cache[\"Z3\"])\n", "        return self.cache[\"A3\"]\n", "\n", "    def backward(self, X, y_true):\n", "        m = X.shape[0]\n", "        A3 = self.cache[\"A3\"]\n", "        dZ3 = A3 - y_true\n", "        dW3 = self.cache[\"A2\"].T @ dZ3 / m\n", "        db3 = np.sum(dZ3, axis=0, keepdims=True) / m\n", "\n", "        dA2 = dZ3 @ self.params[\"W3\"].T\n", "        dZ2 = dA2 * relu_derivative(self.cache[\"Z2\"])\n", "        dW2 = self.cache[\"A1\"].T @ dZ2 / m\n", "        db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n", "\n", "        dA1 = dZ2 @ self.params[\"W2\"].T\n", "        dZ1 = dA1 * relu_derivative(self.cache[\"Z1\"])\n", "        dW1 = X.T @ dZ1 / m\n", "        db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n", "\n", "        for key in [\"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"]:\n", "            self.params[key] -= self.lr * eval(f\"d{key}\")\n", "\n", "    def train_batch(self, X, y):\n", "        y_pred = self.forward(X)\n", "        loss = cross_entropy(y_pred, y)\n", "        acc = accuracy(y_pred, y)\n", "        self.backward(X, y)\n", "        return loss, acc"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.8"}}, "nbformat": 4, "nbformat_minor": 2}